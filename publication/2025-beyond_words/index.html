<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Tahsin Tariq"><meta name=description content="As robots become increasingly integrated into human life, personalizing human-robot interactions (HRI) is crucial for improving user acceptance, engagement, and interaction quality. However, personalizing HRI poses a unique challenge due to the diversity of human personality traits. This paper proposes a method that leverages large language models (LLMs) to dynamically tailor robot conversations according to the Big Five (OCEAN) personality traits. Our novelty lies in using user personality traits to shape robots' verbal responses and implementing contextual action generation for gestures. This study addresses two primary research questions: (1) Does adapting robots' verbal responses based on user personality traits improve communication satisfaction? (2) How does the addition of context-appropriate gestures further enhance user satisfaction? We used Goldberg's personality trait measurement scale (1992) to assess 26 participants who engaged in conversations with an LLM-powered Pepper robot on various topics. The quality of these interactions was self-reported using a revised version of Hecht's (1978) conversation satisfaction scale. Three experimental conditions were conducted: (i) Baseline: Standard LLM conversation, (ii) Personality-congruent: LLM-adjusted dialogue based on personality of participants, and (iii) Enhanced interaction: Personality adaptation plus dynamic gestures. For the third condition, we implemented contextually appropriate pre-defined animations and generated novel gestures by computing joint angle values in real time. Statistical analysis using ANOVA revealed significant differences in communication satisfaction across the three conditions (F=13.41, p<.001). Post-hoc analyses using Šidák's multiple comparison test showed significant pairwise differences: Condition 2 vs. 1: Δ Δmean 4.42, p = 0.02; Condition 3 vs. 1: Δ Δmean 8.23, p < 0.01; Condition 3 vs. 2: Δ Δmean 3.80, p = 0.05. These results demonstrate that both personality-congruent interactions and non-verbal gestures significantly enhance communication satisfaction, with the combined approach yielding the highest satisfaction. This approach opens new possibilities for developing socially intelligent robots with applications in healthcare, education, and customer service."><link rel=alternate hreflang=en-us href=https://tahsintariq.github.io/publication/2025-beyond_words/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.7d31d7a85d84d3fa1c19b09946b72266.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0ce21649deb3bc76a53aa6e76f71d53f_14131_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0ce21649deb3bc76a53aa6e76f71d53f_14131_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://tahsintariq.github.io/publication/2025-beyond_words/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@tariq_tahsin"><meta property="twitter:creator" content="@tariq_tahsin"><meta property="og:site_name" content="Tahsin Tariq Banna"><meta property="og:url" content="https://tahsintariq.github.io/publication/2025-beyond_words/"><meta property="og:title" content="Beyond Words: Integrating Personality Traits and Context-Driven Gestures in Human-Robot Interactions | Tahsin Tariq Banna"><meta property="og:description" content="As robots become increasingly integrated into human life, personalizing human-robot interactions (HRI) is crucial for improving user acceptance, engagement, and interaction quality. However, personalizing HRI poses a unique challenge due to the diversity of human personality traits. This paper proposes a method that leverages large language models (LLMs) to dynamically tailor robot conversations according to the Big Five (OCEAN) personality traits. Our novelty lies in using user personality traits to shape robots' verbal responses and implementing contextual action generation for gestures. This study addresses two primary research questions: (1) Does adapting robots' verbal responses based on user personality traits improve communication satisfaction? (2) How does the addition of context-appropriate gestures further enhance user satisfaction? We used Goldberg's personality trait measurement scale (1992) to assess 26 participants who engaged in conversations with an LLM-powered Pepper robot on various topics. The quality of these interactions was self-reported using a revised version of Hecht's (1978) conversation satisfaction scale. Three experimental conditions were conducted: (i) Baseline: Standard LLM conversation, (ii) Personality-congruent: LLM-adjusted dialogue based on personality of participants, and (iii) Enhanced interaction: Personality adaptation plus dynamic gestures. For the third condition, we implemented contextually appropriate pre-defined animations and generated novel gestures by computing joint angle values in real time. Statistical analysis using ANOVA revealed significant differences in communication satisfaction across the three conditions (F=13.41, p<.001). Post-hoc analyses using Šidák's multiple comparison test showed significant pairwise differences: Condition 2 vs. 1: Δ Δmean 4.42, p = 0.02; Condition 3 vs. 1: Δ Δmean 8.23, p < 0.01; Condition 3 vs. 2: Δ Δmean 3.80, p = 0.05. These results demonstrate that both personality-congruent interactions and non-verbal gestures significantly enhance communication satisfaction, with the combined approach yielding the highest satisfaction. This approach opens new possibilities for developing socially intelligent robots with applications in healthcare, education, and customer service."><meta property="og:image" content="https://tahsintariq.github.io/media/logo_hu05f0f36ce171b899b64ba629782702e3_51428_300x300_fit_lanczos_3.png"><meta property="twitter:image" content="https://tahsintariq.github.io/media/logo_hu05f0f36ce171b899b64ba629782702e3_51428_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-06-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-12T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://tahsintariq.github.io/publication/2025-beyond_words/"},"headline":"Beyond Words: Integrating Personality Traits and Context-Driven Gestures in Human-Robot Interactions","datePublished":"2025-06-05T00:00:00Z","dateModified":"2025-06-12T00:00:00Z","author":{"@type":"Person","name":"**Tahsin Tariq Banna**"},"publisher":{"@type":"Organization","name":"Tahsin Tariq Banna","logo":{"@type":"ImageObject","url":"https://tahsintariq.github.io/media/logo_hu05f0f36ce171b899b64ba629782702e3_51428_192x192_fit_lanczos_3.png"}},"description":"As robots become increasingly integrated into human life, personalizing human-robot interactions (HRI) is crucial for improving user acceptance, engagement, and interaction quality. However, personalizing HRI poses a unique challenge due to the diversity of human personality traits. This paper proposes a method that leverages large language models (LLMs) to dynamically tailor robot conversations according to the Big Five (OCEAN) personality traits. Our novelty lies in using user personality traits to shape robots' verbal responses and implementing contextual action generation for gestures. This study addresses two primary research questions: (1) Does adapting robots' verbal responses based on user personality traits improve communication satisfaction? (2) How does the addition of context-appropriate gestures further enhance user satisfaction? We used Goldberg's personality trait measurement scale (1992) to assess 26 participants who engaged in conversations with an LLM-powered Pepper robot on various topics. The quality of these interactions was self-reported using a revised version of Hecht's (1978) conversation satisfaction scale. Three experimental conditions were conducted: (i) Baseline: Standard LLM conversation, (ii) Personality-congruent: LLM-adjusted dialogue based on personality of participants, and (iii) Enhanced interaction: Personality adaptation plus dynamic gestures. For the third condition, we implemented contextually appropriate pre-defined animations and generated novel gestures by computing joint angle values in real time. Statistical analysis using ANOVA revealed significant differences in communication satisfaction across the three conditions (F=13.41, p\u003c.001). Post-hoc analyses using Šidák's multiple comparison test showed significant pairwise differences: Condition 2 vs. 1: Δ Δmean 4.42, p = 0.02; Condition 3 vs. 1: Δ Δmean 8.23, p \u003c 0.01; Condition 3 vs. 2: Δ Δmean 3.80, p = 0.05. These results demonstrate that both personality-congruent interactions and non-verbal gestures significantly enhance communication satisfaction, with the combined approach yielding the highest satisfaction. This approach opens new possibilities for developing socially intelligent robots with applications in healthcare, education, and customer service."}</script><title>Beyond Words: Integrating Personality Traits and Context-Driven Gestures in Human-Robot Interactions | Tahsin Tariq Banna</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1a2e57f40c0b089fdb796a2b0fa856eb><script src=/js/wowchemy-init.min.1ee5462d74c6c0de1f8881b384ecc58d.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/media/logo_hu05f0f36ce171b899b64ba629782702e3_51428_0x70_resize_lanczos_3.png alt="Tahsin Tariq Banna"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/media/logo_hu05f0f36ce171b899b64ba629782702e3_51428_0x70_resize_lanczos_3.png alt="Tahsin Tariq Banna"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span></span></a></li><li class=nav-item><a class=nav-link href=/><span>Home</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/blog><span>Blogs</span></a></li><li class=nav-item><a class=nav-link href=/files/cv/tahsin_resume.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/tariq_tahsin data-toggle=tooltip data-placement=bottom title="Follow me on Twitter" target=_blank rel=noopener aria-label="Follow me on Twitter"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://github.com/TahsinTariq target=_blank rel=noopener aria-label=github><i class="fab fa-github" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Beyond Words: Integrating Personality Traits and Context-Driven Gestures in Human-Robot Interactions</h1><div class=article-metadata><div><span><strong>Tahsin Tariq Banna</strong></span>, <span>Sejuti Rahman</span>, <span>Mohammad Tareq</span></div><span class=article-date>Posted June, 2025</span>
<span class=article-date></span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://dl.acm.org/doi/10.5555/3709347.3743537 target=_blank rel=noopener><i class="fas fa-scroll mr-1"></i>DOI</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/TahsinTariq/Beyond-Words target=_blank rel=noopener><i class="ai ai-open-data mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header" href=/publication/2025-beyond_words/Tahsin_Beyond_Words.pdf><i class="fas fa-file mr-1"></i>Paper</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>As robots become increasingly integrated into human life, personalizing human-robot interactions (HRI) is crucial for improving user acceptance, engagement, and interaction quality. However, personalizing HRI poses a unique challenge due to the diversity of human personality traits. This paper proposes a method that leverages large language models (LLMs) to dynamically tailor robot conversations according to the Big Five (OCEAN) personality traits. Our novelty lies in using user personality traits to shape robots&rsquo; verbal responses and implementing contextual action generation for gestures. This study addresses two primary research questions: (1) Does adapting robots&rsquo; verbal responses based on user personality traits improve communication satisfaction? (2) How does the addition of context-appropriate gestures further enhance user satisfaction? We used Goldberg&rsquo;s personality trait measurement scale (1992) to assess 26 participants who engaged in conversations with an LLM-powered Pepper robot on various topics. The quality of these interactions was self-reported using a revised version of Hecht&rsquo;s (1978) conversation satisfaction scale. Three experimental conditions were conducted: (i) Baseline: Standard LLM conversation, (ii) Personality-congruent: LLM-adjusted dialogue based on personality of participants, and (iii) Enhanced interaction: Personality adaptation plus dynamic gestures. For the third condition, we implemented contextually appropriate pre-defined animations and generated novel gestures by computing joint angle values in real time. Statistical analysis using ANOVA revealed significant differences in communication satisfaction across the three conditions (F=13.41, p&lt;.001). Post-hoc analyses using Šidák&rsquo;s multiple comparison test showed significant pairwise differences: Condition 2 vs. 1: Δ Δmean 4.42, p = 0.02; Condition 3 vs. 1: Δ Δmean 8.23, p &lt; 0.01; Condition 3 vs. 2: Δ Δmean 3.80, p = 0.05. These results demonstrate that both personality-congruent interactions and non-verbal gestures significantly enhance communication satisfaction, with the combined approach yielding the highest satisfaction. This approach opens new possibilities for developing socially intelligent robots with applications in healthcare, education, and customer service.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS &lsquo;25)</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt="Generated Gestures for the Pepper Robot" srcset="/publication/2025-beyond_words/pepper_aamas_hu222bfde95c4a366f31f1732c12ea0f6f_2042939_fbae34bd91e91c51880cac5da48785d3.webp 400w,
/publication/2025-beyond_words/pepper_aamas_hu222bfde95c4a366f31f1732c12ea0f6f_2042939_086cffeda1bfb24570b038bafa3c066b.webp 760w,
/publication/2025-beyond_words/pepper_aamas_hu222bfde95c4a366f31f1732c12ea0f6f_2042939_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/publication/2025-beyond_words/pepper_aamas_hu222bfde95c4a366f31f1732c12ea0f6f_2042939_fbae34bd91e91c51880cac5da48785d3.webp width=760 height=360 loading=lazy data-zoomable></div></div></figure></p><p style=text-align:center>Figure. The Pepper robot performing generated gestures.</p></div><script src=/%20js/codefolding.js></script>
<link rel=stylesheet href=/%20css/codefolding.css><div class=article-tags><a class="badge badge-light" href=/tag/human-robot-interactions/>Human Robot Interactions</a>
<a class="badge badge-light" href=/tag/large-language-models/>Large Language Models</a>
<a class="badge badge-light" href=/tag/personality-traits/>Personality Traits</a></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 Tahsin Tariq. This work is licensed under <a href=https://creativecommons.org/licenses/by-sa/4.0 rel="noopener noreferrer" target=_blank>CC BY SA 4.0</a>.</p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-sa/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-sa fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Site built with lightly customized version of <a href=https://github.com/simongravelle/simongravelle.github.io target=_blank rel=noopener>Simon Gravelle</a> which itself is customized from <a href=https://github.com/wowchemy/starter-hugo-academic target=_blank rel=noopener>Wowchemy Academic</a>.
Code available on <a href=https://github.com/tahsintariq/tahsintariq.github.io target=_blank rel=noopener>Github</a>.</p><br style=line-height:5px></footer></div></div><script src=/js/vendor-bundle.min.46271ef31da3f018e9cd1b59300aa265.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.a6238d5886fa4a2f7cf92df25709326f.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>